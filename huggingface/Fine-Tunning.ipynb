{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9603ef2c-1ffe-4419-a537-7b7f67a83b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\jayanti.prasad/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\jayanti.prasad/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\jayanti.prasad/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\jayanti.prasad/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\jayanti.prasad/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\jayanti.prasad/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\jayanti.prasad/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\model.safetensors\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\jayanti.prasad\\Anaconda3\\envs\\aiml_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2622: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'text', 'label'], dtype='object') (1000, 3)\n",
      "1000 1000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KerasTensor' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 137\u001b[0m\n\u001b[0;32m    132\u001b[0m target_names \u001b[38;5;241m=\u001b[39m label_class_dict\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    134\u001b[0m train_inp, val_inp, train_label, val_label, train_mask, val_mask \\\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;241m=\u001b[39m train_test_split(input_ids, labels,attention_masks, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m--> 137\u001b[0m M \u001b[38;5;241m=\u001b[39m \u001b[43mBERT_Classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m history \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39mbert_train(workspace, train_inp, train_mask, train_label, val_inp, val_mask, val_label,num_epochs)\n\u001b[0;32m    139\u001b[0m plot_history(history)\n",
      "Cell \u001b[1;32mIn[10], line 56\u001b[0m, in \u001b[0;36mBERT_Classification.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-5\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mbert_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 40\u001b[0m, in \u001b[0;36mbert_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m inps \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(max_len,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     39\u001b[0m masks \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(max_len,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m dbert_layer \u001b[38;5;241m=\u001b[39m \u001b[43mdbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[0;32m     41\u001b[0m dense \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m512\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_regularizer\u001b[38;5;241m=\u001b[39mregularizers\u001b[38;5;241m.\u001b[39ml2(\u001b[38;5;241m0.01\u001b[39m))(dbert_layer)\n\u001b[0;32m     42\u001b[0m dropout \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.5\u001b[39m)(dense)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\aiml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\aiml_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\aiml_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\aiml_env\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:970\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[1;32m--> 970\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KerasTensor' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Dropout, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras import regularizers\n",
    "from transformers import TFDistilBertModel,DistilBertTokenizer,DistilBertConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from text_cleaning import preprocess_sentence\n",
    "\n",
    "max_len, num_classes = 32, 2\n",
    "\n",
    "dbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dbert_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def bert_preproc (sentences):\n",
    "    # Prepare the model input\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        dbert_inps = dbert_tokenizer.encode_plus(sent, add_special_tokens=True, max_length=max_len,\n",
    "                                                 pad_to_max_length=True, return_attention_mask=True, truncation=True)\n",
    "        input_ids.append(dbert_inps['input_ids'])\n",
    "        attention_masks.append(dbert_inps['attention_mask'])\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    attention_masks = np.array(attention_masks)\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "def bert_model ():\n",
    "    inps = Input(shape=(max_len,), dtype='int64')\n",
    "    masks = Input(shape=(max_len,), dtype='int64')\n",
    "    dbert_layer = dbert_model(inps, attention_mask=masks)[0][:, 0, :]\n",
    "    dense = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.01))(dbert_layer)\n",
    "    dropout = Dropout(0.5)(dense)\n",
    "    pred = Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
    "    model = tf.keras.Model(inputs=[inps, masks], outputs=pred)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class BERT_Classification:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "        self.model = bert_model ()\n",
    "\n",
    "\n",
    "    def bert_train (self, workspace, train_inp, train_mask, train_label, val_inp, val_mask, val_label,num_epochs):\n",
    "        log_dir = workspace + os.sep + 'log'\n",
    "    \n",
    "        if os.path.exists(log_dir) and os.path.isdir(log_dir):\n",
    "            shutil.rmtree(log_dir)\n",
    "\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "        model_save_path = workspace + os.sep +  'dbert_model.h5'\n",
    "        chkpt = ModelCheckpoint(filepath = model_save_path, save_weights_only=True, monitor='val_loss',\n",
    "                                               mode='min', save_best_only=True)\n",
    "        tboard = TensorBoard(log_dir=log_dir)\n",
    "        callbacks = [chkpt, tboard]\n",
    "\n",
    "        self.model.compile(loss=self.loss, optimizer=self.optimizer, metrics=[self.metric])\n",
    "        history = self.model.fit([train_inp, train_mask], train_label, batch_size=16, epochs=num_epochs,\n",
    "                            validation_data=([val_inp, val_mask], val_label), callbacks=callbacks)\n",
    "        return history\n",
    "\n",
    "\n",
    "    def bert_predict (self, workspace, val_inp, val_mask):\n",
    "        model_save_path = workspace + os.sep + 'dbert_model.h5'\n",
    "        trained_model = bert_model()\n",
    "        trained_model.compile(loss=self.loss, optimizer=self.optimizer, metrics=[self.metric])\n",
    "        trained_model.load_weights(model_save_path)\n",
    "        preds = trained_model.predict([val_inp, val_mask], batch_size=16)\n",
    "        pred_labels = preds.argmax(axis=1)\n",
    "        return pred_labels, preds.max(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def get_data ():\n",
    "   \n",
    "    file_path = r\"C:\\Users\\jayanti.prasad\\Data\\NLP_DATA\\train\\IMDB_reviews.csv\" \n",
    "\n",
    "    df = pd.read_csv(file_path, encoding='utf-8',nrows=1000)\n",
    "    return df  \n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    axs[1].plot(history.history['accuracy'], '-o', label=\"Training\")\n",
    "    axs[1].plot(history.history['val_accuracy'], '-o', label='Validation')\n",
    "    axs[0].plot(history.history['loss'], '-o', label='Training')\n",
    "    axs[0].plot(history.history['val_loss'], '-o', label='Validation')\n",
    "\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    max_len = 32\n",
    "    num_epochs = 10 \n",
    "\n",
    "    df = get_data() \n",
    "    \n",
    "    print(df.columns, df.shape)\n",
    "    # prepare the data\n",
    "\n",
    "    sentences = df['text'].to_list()\n",
    "    labels = df['label'].to_list()\n",
    "    labels = np.array(labels)\n",
    "    print(len(sentences), len(labels))\n",
    "      \n",
    "    input_ids, attention_masks  = bert_preproc (sentences)\n",
    "\n",
    "    label_class_dict = {0: 'n', 1: 'y'}\n",
    "    \n",
    "    target_names = label_class_dict.values()\n",
    "\n",
    "    train_inp, val_inp, train_label, val_label, train_mask, val_mask \\\n",
    "        = train_test_split(input_ids, labels,attention_masks, test_size=0.2)\n",
    "\n",
    "    M = BERT_Classification()\n",
    "    history = M.bert_train(workspace, train_inp, train_mask, train_label, val_inp, val_mask, val_label,num_epochs)\n",
    "    plot_history(history)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-i','--input-file',help='Input File')\n",
    "    parser.add_argument('-c','--column-name',help='Column Name')\n",
    "    parser.add_argument('-o','--output-dir',help='Output dir')\n",
    "    parser.add_argument('-n','--num_epochs',type=int,help='Number of epochs')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    data = get_data(args.input_file, args.column_name)\n",
    "\n",
    "    data.rename(columns={'COMPLEATION_NOTES': 'text'}, inplace=True)\n",
    "\n",
    "    workspace = args.output_dir\n",
    "\n",
    "    os.makedirs (workspace, exist_ok=True)\n",
    "\n",
    "    #data['gt'] = data['label'].map({'n': 0, 'y': 1})\n",
    "    #print('Available labels: ', data.label.unique())\n",
    "    data['text'] = data['text'].map(preprocess_sentence)\n",
    "    num_classes = len(data.label.unique())\n",
    "\n",
    "    # prepare the data\n",
    "    max_len = 32\n",
    "    sentences = data['text']\n",
    "    labels = data['label'].to_list()\n",
    "    labels = np.array(labels)\n",
    "    print(len(sentences), len(labels))\n",
    "    print(\"labels=\",labels)\n",
    "    #sys.exit()\n",
    "\n",
    "    input_ids, attention_masks  = bert_preproc (sentences)\n",
    "\n",
    "    label_class_dict = {0: 'n', 1: 'y'}\n",
    "    target_names = label_class_dict.values()\n",
    "\n",
    "    train_inp, val_inp, train_label, val_label, train_mask, val_mask \\\n",
    "        = train_test_split(input_ids, labels,attention_masks, test_size=0.2)\n",
    "\n",
    "    M = BERT_Classification()\n",
    "    history = M.bert_train(workspace, train_inp, train_mask, train_label, val_inp, val_mask, val_label,args.num_epochs)\n",
    "    plot_history(history)\n",
    "\n",
    "    lab_p, prob_p = M.bert_predict(workspace, val_inp, val_mask)\n",
    "\n",
    "    print(lab_p)\n",
    "    print(prob_p)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49908861-3f62-4108-a79c-810cbff0a31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
